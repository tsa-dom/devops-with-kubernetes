Script started on 2021-12-14 14:25:02+02:00 [TERM="xterm-256color" TTY="/dev/pts/9" COLUMNS="80" LINES="24"]
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl apply -k github.com/fluxcd/flagger/kustomize/linkerd
customresourcedefinition.apiextensions.k8s.io/alertproviders.flagger.app created
customresourcedefinition.apiextensions.k8s.io/canaries.flagger.app created
customresourcedefinition.apiextensions.k8s.io/metrictemplates.flagger.app created
serviceaccount/flagger created
clusterrole.rbac.authorization.k8s.io/flagger created
clusterrolebinding.rbac.authorization.k8s.io/flagger created
deployment.apps/flagger created
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n linkerd rollout status deploy/flagger
deployment "flagger" successfully rolled out
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ [K]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl create ns test && \
>   kubectl apply -f https://run.linkerd.io/flagger.yml
namespace/test created
deployment.apps/load created
configmap/frontend created
deployment.apps/frontend created
service/frontend created
deployment.apps/podinfo created
service/podinfo created
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test rollout status deploy podinfo
deployment "podinfo" successfully rolled out
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test port-forward svc/frontend 8080
Forwarding from 127.0.0.1:8080 -> 8080
Forwarding from [::1]:8080 -> 8080
Handling connection for 8080
Handling connection for 8080
Handling connection for 8080
^C]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ cat <<EOF | kubectl apply -f -
> apiVersion: flagger.app/v1beta1
> kind: Canary
> metadata:
>   name: podinfo
>   namespace: test
> spec:
>   targetRef:
>     apiVersion: apps/v1
>     kind: Deployment
>     name: podinfo
>   service:
>     port: 9898
>   analysis:
>     interval: 10s
>     threshold: 5
>     stepWeight: 10
>     maxWeight: 100
>     metrics:
>     - name: request-success-rate
>       thresholdRange:
>         min: 99
>       interval: 1m
>     - name: request-duration
>       thresholdRange:
>         max: 500
>       interval: 1m
> EOF
canary.flagger.app/podinfo created
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get ev --watch
LAST SEEN   TYPE      REASON                  OBJECT                                  MESSAGE
5m43s       Normal    ScalingReplicaSet       deployment/load                         Scaled up replica set load-7f97579865 to 1
5m43s       Normal    ScalingReplicaSet       deployment/frontend                     Scaled up replica set frontend-6957977dc7 to 1
5m43s       Normal    Injected                deployment/load                         Linkerd sidecar proxy injected
5m43s       Normal    Injected                deployment/frontend                     Linkerd sidecar proxy injected
5m43s       Normal    ScalingReplicaSet       deployment/podinfo                      Scaled up replica set podinfo-7bfd46f477 to 1
5m43s       Normal    SuccessfulCreate        replicaset/load-7f97579865              Created pod: load-7f97579865-hxh8x
5m43s       Normal    Injected                deployment/podinfo                      Linkerd sidecar proxy injected
5m42s       Normal    Scheduled               pod/frontend-6957977dc7-fp5lw           Successfully assigned test/frontend-6957977dc7-fp5lw to k3d-k3s-default-server-0
5m42s       Normal    Scheduled               pod/load-7f97579865-hxh8x               Successfully assigned test/load-7f97579865-hxh8x to k3d-k3s-default-agent-1
5m43s       Normal    SuccessfulCreate        replicaset/frontend-6957977dc7          Created pod: frontend-6957977dc7-fp5lw
5m43s       Normal    SuccessfulCreate        replicaset/podinfo-7bfd46f477           Created pod: podinfo-7bfd46f477-pwdql
5m42s       Normal    Scheduled               pod/podinfo-7bfd46f477-pwdql            Successfully assigned test/podinfo-7bfd46f477-pwdql to k3d-k3s-default-agent-1
5m42s       Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
5m42s       Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container linkerd-init
5m42s       Normal    Pulled                  pod/load-7f97579865-hxh8x               Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
5m42s       Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
5m42s       Normal    Created                 pod/load-7f97579865-hxh8x               Created container linkerd-init
5m42s       Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container linkerd-init
5m42s       Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container linkerd-init
5m42s       Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container linkerd-init
5m42s       Normal    Started                 pod/load-7f97579865-hxh8x               Started container linkerd-init
5m41s       Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
5m41s       Normal    Pulled                  pod/load-7f97579865-hxh8x               Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
5m41s       Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
5m41s       Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container linkerd-proxy
5m41s       Normal    Created                 pod/load-7f97579865-hxh8x               Created container linkerd-proxy
5m41s       Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container linkerd-proxy
5m41s       Normal    Started                 pod/load-7f97579865-hxh8x               Started container linkerd-proxy
5m41s       Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container linkerd-proxy
5m41s       Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container linkerd-proxy
5m41s       Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: a933bf3a0d277fea21647aca4716716c
5m41s       Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: 374122516c2110e43ba2b78492808fac
5m41s       Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: 2942414938187e02cef9d7e45b148fbd
5m40s       Normal    Pulling                 pod/frontend-6957977dc7-fp5lw           Pulling image "nginx:alpine"
5m40s       Normal    Pulling                 pod/load-7f97579865-hxh8x               Pulling image "buoyantio/slow_cooker:1.2.0"
5m40s       Normal    Pulling                 pod/podinfo-7bfd46f477-pwdql            Pulling image "quay.io/stefanprodan/podinfo:1.7.0"
5m36s       Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.0" in 3.829840887s
5m36s       Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container podinfod
5m36s       Normal    Pulled                  pod/load-7f97579865-hxh8x               Successfully pulled image "buoyantio/slow_cooker:1.2.0" in 3.932179578s
5m36s       Normal    Created                 pod/load-7f97579865-hxh8x               Created container slow-cooker
5m36s       Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container podinfod
5m36s       Normal    Started                 pod/load-7f97579865-hxh8x               Started container slow-cooker
5m36s       Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Successfully pulled image "nginx:alpine" in 4.617016122s
5m35s       Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container nginx
5m35s       Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container nginx
4s          Normal    Synced                  canary/podinfo                          all the metrics providers are available!
4s          Warning   Synced                  canary/podinfo                          podinfo-primary.test not ready: waiting for rollout to finish: observed deployment generation less than desired generation
4s          Normal    ScalingReplicaSet       deployment/podinfo-primary              Scaled up replica set podinfo-primary-55dbbfc9c8 to 1
4s          Normal    Injected                deployment/podinfo-primary              Linkerd sidecar proxy injected
4s          Normal    SuccessfulCreate        replicaset/podinfo-primary-55dbbfc9c8   Created pod: podinfo-primary-55dbbfc9c8-k66f2
4s          Normal    Scheduled               pod/podinfo-primary-55dbbfc9c8-k66f2    Successfully assigned test/podinfo-primary-55dbbfc9c8-k66f2 to k3d-k3s-default-server-0
1s          Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
1s          Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container linkerd-init
1s          Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container linkerd-init
1s          Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
1s          Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container linkerd-proxy
1s          Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container linkerd-proxy
1s          Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:35:13 +0000 UTC: 7e2030cbd401f7881a27b7babc18bfec
0s          Normal    Pulling                 pod/podinfo-primary-55dbbfc9c8-k66f2    Pulling image "quay.io/stefanprodan/podinfo:1.7.0"
0s          Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.0" in 3.484891178s
0s          Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container podinfod
0s          Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container podinfod
0s          Normal    Synced                  canary/podinfo                          all the metrics providers are available!
0s          Normal    ScalingReplicaSet       deployment/podinfo                      Scaled down replica set podinfo-7bfd46f477 to 0
0s          Normal    SuccessfulDelete        replicaset/podinfo-7bfd46f477           Deleted pod: podinfo-7bfd46f477-pwdql
0s          Normal    Killing                 pod/podinfo-7bfd46f477-pwdql            Stopping container linkerd-proxy
0s          Normal    Killing                 pod/podinfo-7bfd46f477-pwdql            Stopping container podinfod
0s          Normal    Synced                  canary/podinfo                          Initialization done! podinfo.test
0s          Warning   Unhealthy               pod/podinfo-7bfd46f477-pwdql            Readiness probe failed: Get "http://10.42.0.39:4191/ready": dial tcp 10.42.0.39:4191: connect: connection refused
0s          Warning   Unhealthy               pod/podinfo-7bfd46f477-pwdql            Liveness probe failed: Get "http://10.42.0.39:4191/live": dial tcp 10.42.0.39:4191: connect: connection refused
^C]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get svc
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
frontend          ClusterIP   10.43.40.234    <none>        8080/TCP   7m36s
podinfo-canary    ClusterIP   10.43.243.242   <none>        9898/TCP   117s
podinfo-primary   ClusterIP   10.43.96.255    <none>        9898/TCP   117s
podinfo           ClusterIP   10.43.164.178   <none>        9898/TCP   7m36s
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test set image deployment/podinfo \
>   podinfod=quay.io/stefanprodan/podinfo:1.7.1
deployment.apps/podinfo image updated
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get ev --watch
LAST SEEN   TYPE      REASON                  OBJECT                                  MESSAGE
12m         Normal    ScalingReplicaSet       deployment/load                         Scaled up replica set load-7f97579865 to 1
12m         Normal    ScalingReplicaSet       deployment/frontend                     Scaled up replica set frontend-6957977dc7 to 1
12m         Normal    Injected                deployment/load                         Linkerd sidecar proxy injected
12m         Normal    Injected                deployment/frontend                     Linkerd sidecar proxy injected
12m         Normal    ScalingReplicaSet       deployment/podinfo                      Scaled up replica set podinfo-7bfd46f477 to 1
12m         Normal    SuccessfulCreate        replicaset/load-7f97579865              Created pod: load-7f97579865-hxh8x
12m         Normal    Scheduled               pod/frontend-6957977dc7-fp5lw           Successfully assigned test/frontend-6957977dc7-fp5lw to k3d-k3s-default-server-0
12m         Normal    Scheduled               pod/load-7f97579865-hxh8x               Successfully assigned test/load-7f97579865-hxh8x to k3d-k3s-default-agent-1
12m         Normal    SuccessfulCreate        replicaset/frontend-6957977dc7          Created pod: frontend-6957977dc7-fp5lw
12m         Normal    SuccessfulCreate        replicaset/podinfo-7bfd46f477           Created pod: podinfo-7bfd46f477-pwdql
12m         Normal    Scheduled               pod/podinfo-7bfd46f477-pwdql            Successfully assigned test/podinfo-7bfd46f477-pwdql to k3d-k3s-default-agent-1
12m         Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
12m         Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container linkerd-init
12m         Normal    Pulled                  pod/load-7f97579865-hxh8x               Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
12m         Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
12m         Normal    Created                 pod/load-7f97579865-hxh8x               Created container linkerd-init
12m         Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container linkerd-init
12m         Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container linkerd-init
12m         Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container linkerd-init
12m         Normal    Started                 pod/load-7f97579865-hxh8x               Started container linkerd-init
12m         Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
12m         Normal    Pulled                  pod/load-7f97579865-hxh8x               Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
12m         Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
12m         Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container linkerd-proxy
12m         Normal    Created                 pod/load-7f97579865-hxh8x               Created container linkerd-proxy
12m         Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container linkerd-proxy
12m         Normal    Started                 pod/load-7f97579865-hxh8x               Started container linkerd-proxy
12m         Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container linkerd-proxy
12m         Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container linkerd-proxy
12m         Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: a933bf3a0d277fea21647aca4716716c
12m         Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: 374122516c2110e43ba2b78492808fac
12m         Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:29:33 +0000 UTC: 2942414938187e02cef9d7e45b148fbd
12m         Normal    Pulling                 pod/frontend-6957977dc7-fp5lw           Pulling image "nginx:alpine"
12m         Normal    Pulling                 pod/load-7f97579865-hxh8x               Pulling image "buoyantio/slow_cooker:1.2.0"
12m         Normal    Pulling                 pod/podinfo-7bfd46f477-pwdql            Pulling image "quay.io/stefanprodan/podinfo:1.7.0"
12m         Normal    Pulled                  pod/podinfo-7bfd46f477-pwdql            Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.0" in 3.829840887s
12m         Normal    Created                 pod/podinfo-7bfd46f477-pwdql            Created container podinfod
12m         Normal    Pulled                  pod/load-7f97579865-hxh8x               Successfully pulled image "buoyantio/slow_cooker:1.2.0" in 3.932179578s
12m         Normal    Created                 pod/load-7f97579865-hxh8x               Created container slow-cooker
12m         Normal    Started                 pod/podinfo-7bfd46f477-pwdql            Started container podinfod
12m         Normal    Started                 pod/load-7f97579865-hxh8x               Started container slow-cooker
12m         Normal    Pulled                  pod/frontend-6957977dc7-fp5lw           Successfully pulled image "nginx:alpine" in 4.617016122s
12m         Normal    Created                 pod/frontend-6957977dc7-fp5lw           Created container nginx
12m         Normal    Started                 pod/frontend-6957977dc7-fp5lw           Started container nginx
7m18s       Warning   Synced                  canary/podinfo                          podinfo-primary.test not ready: waiting for rollout to finish: observed deployment generation less than desired generation
7m18s       Normal    ScalingReplicaSet       deployment/podinfo-primary              Scaled up replica set podinfo-primary-55dbbfc9c8 to 1
7m18s       Normal    Injected                deployment/podinfo-primary              Linkerd sidecar proxy injected
7m18s       Normal    SuccessfulCreate        replicaset/podinfo-primary-55dbbfc9c8   Created pod: podinfo-primary-55dbbfc9c8-k66f2
7m17s       Normal    Scheduled               pod/podinfo-primary-55dbbfc9c8-k66f2    Successfully assigned test/podinfo-primary-55dbbfc9c8-k66f2 to k3d-k3s-default-server-0
7m15s       Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
7m15s       Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container linkerd-init
7m15s       Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container linkerd-init
7m15s       Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
7m15s       Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container linkerd-proxy
7m15s       Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container linkerd-proxy
7m15s       Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:35:13 +0000 UTC: 7e2030cbd401f7881a27b7babc18bfec
7m14s       Normal    Pulling                 pod/podinfo-primary-55dbbfc9c8-k66f2    Pulling image "quay.io/stefanprodan/podinfo:1.7.0"
7m10s       Normal    Pulled                  pod/podinfo-primary-55dbbfc9c8-k66f2    Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.0" in 3.484891178s
7m10s       Normal    Created                 pod/podinfo-primary-55dbbfc9c8-k66f2    Created container podinfod
7m10s       Normal    Started                 pod/podinfo-primary-55dbbfc9c8-k66f2    Started container podinfod
7m8s        Normal    Synced                  canary/podinfo                          all the metrics providers are available!
7m8s        Normal    ScalingReplicaSet       deployment/podinfo                      Scaled down replica set podinfo-7bfd46f477 to 0
7m8s        Normal    SuccessfulDelete        replicaset/podinfo-7bfd46f477           Deleted pod: podinfo-7bfd46f477-pwdql
7m8s        Normal    Killing                 pod/podinfo-7bfd46f477-pwdql            Stopping container linkerd-proxy
7m8s        Normal    Killing                 pod/podinfo-7bfd46f477-pwdql            Stopping container podinfod
7m8s        Normal    Synced                  canary/podinfo                          Initialization done! podinfo.test
7m7s        Warning   Unhealthy               pod/podinfo-7bfd46f477-pwdql            Readiness probe failed: Get "http://10.42.0.39:4191/ready": dial tcp 10.42.0.39:4191: connect: connection refused
7m7s        Warning   Unhealthy               pod/podinfo-7bfd46f477-pwdql            Liveness probe failed: Get "http://10.42.0.39:4191/live": dial tcp 10.42.0.39:4191: connect: connection refused
108s        Normal    Synced                  canary/podinfo                          New revision detected! Scaling up podinfo.test
108s        Normal    ScalingReplicaSet       deployment/podinfo                      Scaled up replica set podinfo-69c49997fd to 1
108s        Normal    Injected                deployment/podinfo                      Linkerd sidecar proxy injected
108s        Normal    SuccessfulCreate        replicaset/podinfo-69c49997fd           Created pod: podinfo-69c49997fd-szxrr
107s        Normal    Scheduled               pod/podinfo-69c49997fd-szxrr            Successfully assigned test/podinfo-69c49997fd-szxrr to k3d-k3s-default-agent-1
105s        Normal    Pulled                  pod/podinfo-69c49997fd-szxrr            Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
105s        Normal    Created                 pod/podinfo-69c49997fd-szxrr            Created container linkerd-init
105s        Normal    Started                 pod/podinfo-69c49997fd-szxrr            Started container linkerd-init
104s        Normal    Pulled                  pod/podinfo-69c49997fd-szxrr            Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
104s        Normal    Created                 pod/podinfo-69c49997fd-szxrr            Created container linkerd-proxy
104s        Normal    Started                 pod/podinfo-69c49997fd-szxrr            Started container linkerd-proxy
104s        Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:40:44 +0000 UTC: 481249e556643207e012c77707e6e61d
104s        Normal    Pulling                 pod/podinfo-69c49997fd-szxrr            Pulling image "quay.io/stefanprodan/podinfo:1.7.1"
100s        Normal    Pulled                  pod/podinfo-69c49997fd-szxrr            Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.1" in 3.401897192s
100s        Normal    Created                 pod/podinfo-69c49997fd-szxrr            Created container podinfod
100s        Normal    Started                 pod/podinfo-69c49997fd-szxrr            Started container podinfod
98s         Normal    Synced                  canary/podinfo                          Starting canary analysis for podinfo.test
98s         Normal    Synced                  canary/podinfo                          Advance podinfo.test canary weight 10
88s         Normal    Synced                  canary/podinfo                          Advance podinfo.test canary weight 20
78s         Normal    Synced                  canary/podinfo                          Advance podinfo.test canary weight 30
68s         Normal    Synced                  canary/podinfo                          Advance podinfo.test canary weight 40
58s         Normal    Synced                  canary/podinfo                          Advance podinfo.test canary weight 50
8s          Normal    Synced                  canary/podinfo                          (combined from similar events): Advance podinfo.test canary weight 100
0s          Normal    Synced                  canary/podinfo                          (combined from similar events): Copying podinfo.test template spec to podinfo-primary.test
0s          Normal    ScalingReplicaSet       deployment/podinfo-primary              Scaled up replica set podinfo-primary-7ff5c594ff to 1
0s          Normal    Injected                deployment/podinfo-primary              Linkerd sidecar proxy injected
0s          Normal    SuccessfulCreate        replicaset/podinfo-primary-7ff5c594ff   Created pod: podinfo-primary-7ff5c594ff-7pfwl
0s          Normal    Scheduled               pod/podinfo-primary-7ff5c594ff-7pfwl    Successfully assigned test/podinfo-primary-7ff5c594ff-7pfwl to k3d-k3s-default-agent-0
0s          Normal    Pulled                  pod/podinfo-primary-7ff5c594ff-7pfwl    Container image "cr.l5d.io/linkerd/proxy-init:v1.4.0" already present on machine
0s          Normal    Created                 pod/podinfo-primary-7ff5c594ff-7pfwl    Created container linkerd-init
0s          Normal    Started                 pod/podinfo-primary-7ff5c594ff-7pfwl    Started container linkerd-init
0s          Normal    Pulled                  pod/podinfo-primary-7ff5c594ff-7pfwl    Container image "cr.l5d.io/linkerd/proxy:stable-2.11.1" already present on machine
0s          Normal    Created                 pod/podinfo-primary-7ff5c594ff-7pfwl    Created container linkerd-proxy
0s          Normal    Started                 pod/podinfo-primary-7ff5c594ff-7pfwl    Started container linkerd-proxy
0s          Normal    IssuedLeafCertificate   serviceaccount/default                  issued certificate for default.test.serviceaccount.identity.linkerd.cluster.local until 2021-12-15 12:42:34 +0000 UTC: 82b352d487dcc9a8a651b3d557ec0b3b
0s          Normal    Pulling                 pod/podinfo-primary-7ff5c594ff-7pfwl    Pulling image "quay.io/stefanprodan/podinfo:1.7.1"
0s          Normal    Pulled                  pod/podinfo-primary-7ff5c594ff-7pfwl    Successfully pulled image "quay.io/stefanprodan/podinfo:1.7.1" in 3.626508986s
0s          Normal    Created                 pod/podinfo-primary-7ff5c594ff-7pfwl    Created container podinfod
0s          Normal    Started                 pod/podinfo-primary-7ff5c594ff-7pfwl    Started container podinfod
0s          Normal    ScalingReplicaSet       deployment/podinfo-primary              Scaled down replica set podinfo-primary-55dbbfc9c8 to 0
0s          Normal    SuccessfulDelete        replicaset/podinfo-primary-55dbbfc9c8   Deleted pod: podinfo-primary-55dbbfc9c8-k66f2
0s          Normal    Killing                 pod/podinfo-primary-55dbbfc9c8-k66f2    Stopping container linkerd-proxy
0s          Normal    Killing                 pod/podinfo-primary-55dbbfc9c8-k66f2    Stopping container podinfod
0s          Normal    Synced                  canary/podinfo                          (combined from similar events): Routing all traffic to primary
0s          Warning   Unhealthy               pod/podinfo-primary-55dbbfc9c8-k66f2    Liveness probe failed: Get "http://10.42.1.42:4191/live": dial tcp 10.42.1.42:4191: connect: connection refused
0s          Warning   Unhealthy               pod/podinfo-primary-55dbbfc9c8-k66f2    Readiness probe failed: Get "http://10.42.1.42:4191/ready": dial tcp 10.42.1.42:4191: connect: connection refused
0s          Normal    Synced                  canary/podinfo                          (combined from similar events): Promotion completed! Scaling down podinfo.test
0s          Normal    ScalingReplicaSet       deployment/podinfo                      Scaled down replica set podinfo-69c49997fd to 0
0s          Normal    SuccessfulDelete        replicaset/podinfo-69c49997fd           Deleted pod: podinfo-69c49997fd-szxrr
0s          Normal    Killing                 pod/podinfo-69c49997fd-szxrr            Stopping container linkerd-proxy
0s          Normal    Killing                 pod/podinfo-69c49997fd-szxrr            Stopping container podinfod
0s          Warning   Unhealthy               pod/podinfo-69c49997fd-szxrr            Readiness probe failed: Get "http://10.42.0.40:4191/ready": dial tcp 10.42.0.40:4191: connect: connection refused
0s          Warning   Unhealthy               pod/podinfo-69c49997fd-szxrr            Liveness probe failed: Get "http://10.42.0.40:4191/live": dial tcp 10.42.0.40:4191: connect: connection refused
^C]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get ev --watch[Kwatch kubectl -n test get canary
[?1049h[22;0;0t[1;34r(B[m[4l[?7h[H[2JEvery 2,0s: kubectl -n test get canary[1;176Htapio-desktop: Tue Dec 14 14:44:07 2021[3;1HNAME[11GSTATUS[23GWEIGHT   LASTTRANSITIONTIME[4dpodinfo   Succeeded   0[4;32H2021-12-14T12:42:30Z[34;214H[1;209H9[34;214H[1;208H11[34;214H[1;209H4[34;214H[1;209H6[34;214H[1;209H8[34;214H[1;208H20[34;214H[1;209H2[34;214H[1;209H4[34;214H[1;209H6[34;214H[1;209H8[34;214H[1;208H30[34;214H[1;209H2[34;214H[1;209H4[34;214H[1;209H6[34;214H[1;209H8[34;214H[1;208H40[34;214H[1;209H2[34;214H[1;209H4[34;214H[1;209H6[34;214H[1;209H8[34;214H[1;208H51[34;214H[1;209H3[34;214H[34;1H[?1049l[23;0;0t[?1l>]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 12
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16291"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "0"
  - service: podinfo-primary
    weight: "100"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test set image deployment/podinfo \
>   podinfod=quay.io/stefanprodan/podinfo:1.7.1[K0
deployment.apps/podinfo image updated
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test set image deployment/podinfo   podinfod=quay.io/stefanprodan/podinfo:1.7.0[42Pget trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 12
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16291"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "0"
  - service: podinfo-primary
    weight: "100"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 13
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16693"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "10"
  - service: podinfo-primary
    weight: "90"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml[Kkubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 14
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16727"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "20"
  - service: podinfo-primary
    weight: "80"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 15
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16745"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "30"
  - service: podinfo-primary
    weight: "70"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 16
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16762"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "40"
  - service: podinfo-primary
    weight: "60"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 18
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16796"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "60"
  - service: podinfo-primary
    weight: "40"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 19
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16812"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "70"
  - service: podinfo-primary
    weight: "30"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 22
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16861"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "100"
  - service: podinfo-primary
    weight: "0"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl -n test get trafficsplit podinfo -o yaml
apiVersion: split.smi-spec.io/v1alpha2
kind: TrafficSplit
metadata:
  creationTimestamp: "2021-12-14T12:35:00Z"
  generation: 23
  name: podinfo
  namespace: test
  ownerReferences:
  - apiVersion: flagger.app/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: Canary
    name: podinfo
    uid: 27d06281-a6e4-4672-9e6b-a53b442a246e
  resourceVersion: "16943"
  uid: aa3c0054-8fb7-4f9e-ad6b-b2ccebdaff90
spec:
  backends:
  - service: podinfo-canary
    weight: "0"
  - service: podinfo-primary
    weight: "100"
  service: podinfo
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ watch linkerd viz -n test stat deploy --from deploy/load
[?1049h[22;0;0t[1;34r(B[m[4l[?7h[H[2JEvery 2,0s: linkerd viz -n test stat deploy --from deploy/load[1;176Htapio-desktop: Tue Dec 14 14:48:44 2021[3;1HNAME[3;19HMESHED   SUCCESS[41GRPS   LATENCY_P50   LATENCY_P95   LATENCY_P99   TCP_CONN[4dpodinfo[4;22H0/0   100.00%   4.8rps[4;55H0ms[4;69H0ms[4;83H0ms[4;96H0[5dpodinfo-primary[22G1/1   100.00%   5.9rps[5;55H1ms[5;69H1ms[5;83H1ms[5;96H1[34;214H[1;209H6[4;38H2.9[5;38H6.3[34;214H[1;209H9[5;40H7[34;214H[1;208H52[5;38H7.2[34;214H[1;209H4[4;38H0[5d.6[34;214H[1;209H7[5;38H8.1[34;214H[1;206H9:00[5;40H5[34;214H[1;209H3[5;38H9.0[34;214H[1;209H5[4;28H      -        -[4;55H  -[4;69H  -[4;83H  -[4;96H-[5;40H4[34;214H[1;209H8[5;40H8[34;214H[1;208H11[3;35H[41G RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99   TCP_CONN[4;35H[4;43H -[4;57H -[4;71H -[4;85H -[4;96H -[5;38H10.0rps[5;55H 1ms[5;69H 1ms[5;83H 1ms[5;96H 1[34;214H[1;209H3[34;214H[1;209H6[34;214H[1;209H9[34;214H[1;208H21[34;214H[1;209H4[34;214H[1;209H7[34;214H[1;208H30[34;214H[1;209H2[34;214H[1;209H5[34;214H[1;209H8[34;214H[1;208H40[34;214H[1;209H3[34;214H[1;209H6[34;214H[1;209H8[34;214H[1;208H51[34;214H[1;209H4[34;214H[1;209H7[34;214H[1;209H9[34;214H[1;205H50:02[34;214H[1;209H5[34;214H[1;209H7[34;214H[1;208H10[34;214H[1;209H3[34;214H[1;209H5[34;214H[1;209H8[34;214H[1;208H21[34;214H[1;209H4[34;214H[34;1H[?1049l[23;0;0t[?1l>]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ linkerd viz dashboard
Error running port-forward: unable to listen on any of the requested ports: [{50750 8084}] for linkerd-viz/web-db97ff489-sfm5j
Check for `linkerd dashboard` running in other terminal sessions, or use the `--port` flag.
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ linkerd viz dashboard --port 8080
Linkerd dashboard available at:
http://localhost:8080
Grafana dashboard available at:
http://localhost:8080/grafana
Opening Linkerd dashboard in the default browser
Opening in existing browser session.
mesa: for the --simplifycfg-sink-common option: may only occur zero or one times!
mesa: for the --global-isel-abort option: may only occur zero or one times!
mesa: for the --amdgpu-atomic-optimizations option: may only occur zero or one times!
mesa: for the --structurizecfg-skip-uniform-regions option: may only occur zero or one times!
[2467358:2467358:0100/000000.078323:ERROR:sandbox_linux.cc(376)] InitializeSandbox() called with multiple threads in process gpu-process.
^C]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ linkerd viz dashboard --port 8080[K[K79
Linkerd dashboard available at:
http://localhost:8079
Grafana dashboard available at:
http://localhost:8079/grafana
Opening Linkerd dashboard in the default browser
Opening in existing browser session.
mesa: for the --simplifycfg-sink-common option: may only occur zero or one times!
mesa: for the --global-isel-abort option: may only occur zero or one times!
mesa: for the --amdgpu-atomic-optimizations option: may only occur zero or one times!
mesa: for the --structurizecfg-skip-uniform-regions option: may only occur zero or one times!
[2476710:2476710:0100/000000.627896:ERROR:sandbox_linux.cc(376)] InitializeSandbox() called with multiple threads in process gpu-process.
^C]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ curl http://locahost:8080
curl: (6) Could not resolve host: locahost
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ curl http://locahost:8080[C[Clhost:8080
{
  "hostname": "podinfo-primary-6cf55c7988-g6ptb",
  "version": "1.7.0",
  "revision": "4fc593f42c7cd2e7319c83f6bfd3743c05523883",
  "color": "blue",
  "message": "greetings from podinfo v1.7.0",
  "goos": "linux",
  "goarch": "amd64",
  "runtime": "go1.11.2",
  "num_goroutine": "7",
  "num_cpu": "16"
}]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ kubectl delete -k github.com/fluxcd/flagger/kustomize/linkerd && \
>   kubectl delete ns test
customresourcedefinition.apiextensions.k8s.io "alertproviders.flagger.app" deleted
customresourcedefinition.apiextensions.k8s.io "canaries.flagger.app" deleted
customresourcedefinition.apiextensions.k8s.io "metrictemplates.flagger.app" deleted
serviceaccount "flagger" deleted
clusterrole.rbac.authorization.k8s.io "flagger" deleted
clusterrolebinding.rbac.authorization.k8s.io "flagger" deleted
deployment.apps "flagger" deleted
namespace "test" deleted
]0;tapio@tapio-desktop: ~/Documents/GitRepositories/KubeProject[01;32mtapio@tapio-desktop[00m:[01;34m~/Documents/GitRepositories/KubeProject[00m$ exit
exit

Script done on 2021-12-14 14:59:08+02:00 [COMMAND_EXIT_CODE="0"]
